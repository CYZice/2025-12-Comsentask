#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YOLOæ ‡æ³¨æ–‡ä»¶éªŒè¯å·¥å…·
ç”¨äºéªŒè¯æ ‡æ³¨æ–‡ä»¶æ˜¯å¦ç¬¦åˆå•ç±»åˆ«ç›®æ ‡æ£€æµ‹è¦æ±‚
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path ä»¥æ”¯æŒç»å¯¹å¯¼å…¥
project_root = Path(__file__).resolve().parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import os
import glob
from typing import List, Tuple, Dict, Optional


def validate_annotation_file(file_path: str) -> Tuple[bool, List[str], Dict[str, int]]:
    """
    éªŒè¯å•ä¸ªæ ‡æ³¨æ–‡ä»¶ã€‚
    
    Args:
        file_path (str): æ ‡æ³¨æ–‡ä»¶è·¯å¾„ã€‚
        
    Returns:
        Tuple[bool, List[str], Dict[str, int]]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
    """
    errors = []
    stats = {
        'total_lines': 0,
        'valid_lines': 0,
        'class_ids': {},
        'invalid_format_lines': 0,
        'empty_lines': 0
    }
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        stats['total_lines'] = len(lines)
        
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            
            if not line:
                stats['empty_lines'] += 1
                continue
            
            parts = line.split()
            
            # æ£€æŸ¥æ ¼å¼
            if len(parts) < 5:
                errors.append(f"{os.path.basename(file_path)}:{line_num} - æ ¼å¼é”™è¯¯ï¼ˆéœ€è¦è‡³å°‘5ä¸ªå€¼ï¼Œå½“å‰{len(parts)}ä¸ªï¼‰")
                stats['invalid_format_lines'] += 1
                continue
            
            try:
                class_id = int(parts[0])
                
                # æ£€æŸ¥ç±»åˆ«ID
                if class_id < 0:
                    errors.append(f"{os.path.basename(file_path)}:{line_num} - ç±»åˆ«IDä¸èƒ½ä¸ºè´Ÿæ•°: {class_id}")
                    continue
                
                # è®°å½•ç±»åˆ«IDç»Ÿè®¡
                stats['class_ids'][str(class_id)] = stats['class_ids'].get(str(class_id), 0) + 1
                
                # æ£€æŸ¥åæ ‡å€¼
                x_center = float(parts[1])
                y_center = float(parts[2])
                width = float(parts[3])
                height = float(parts[4])
                
                # æ£€æŸ¥åæ ‡èŒƒå›´ï¼ˆYOLOæ ¼å¼åº”è¯¥åœ¨0-1ä¹‹é—´ï¼‰
                if not (0 <= x_center <= 1):
                    errors.append(f"{os.path.basename(file_path)}:{line_num} - x_centerè¶…å‡ºèŒƒå›´: {x_center}")
                    continue
                
                if not (0 <= y_center <= 1):
                    errors.append(f"{os.path.basename(file_path)}:{line_num} - y_centerè¶…å‡ºèŒƒå›´: {y_center}")
                    continue
                
                if not (0 <= width <= 1):
                    errors.append(f"{os.path.basename(file_path)}:{line_num} - widthè¶…å‡ºèŒƒå›´: {width}")
                    continue
                
                if not (0 <= height <= 1):
                    errors.append(f"{os.path.basename(file_path)}:{line_num} - heightè¶…å‡ºèŒƒå›´: {height}")
                    continue
                
                stats['valid_lines'] += 1
                
            except ValueError as e:
                errors.append(f"{os.path.basename(file_path)}:{line_num} - æ•°å€¼è½¬æ¢é”™è¯¯: {e}")
                stats['invalid_format_lines'] += 1
                continue
            
    except Exception as e:
        errors.append(f"{os.path.basename(file_path)} - æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
        return False, errors, stats
    
    return len(errors) == 0, errors, stats


def validate_dataset_directory(dataset_path: str, expected_class_id: Optional[int] = 0) -> Tuple[bool, List[str], Dict[str, any]]:
    """
    éªŒè¯æ•´ä¸ªæ•°æ®é›†çš„æ ‡æ³¨æ–‡ä»¶ã€‚
    
    Args:
        dataset_path (str): æ•°æ®é›†è·¯å¾„ã€‚
        expected_class_id (Optional[int]): æœŸæœ›çš„ç±»åˆ«IDï¼Œé»˜è®¤ä¸º0ï¼ˆå•ç±»åˆ«ï¼‰ã€‚
        
    Returns:
        Tuple[bool, List[str], Dict[str, any]]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯åˆ—è¡¨, æ±‡æ€»ç»Ÿè®¡)
    """
    errors = []
    summary = {
        'total_files': 0,
        'valid_files': 0,
        'invalid_files': 0,
        'total_objects': 0,
        'class_distribution': {},
        'files_with_wrong_class': [],
        'empty_files': []
    }
    
    # æŸ¥æ‰¾æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
    label_dirs = ['labels/train', 'labels/val']
    all_label_files = []
    
    for label_dir in label_dirs:
        label_path = os.path.join(dataset_path, label_dir)
        if os.path.exists(label_path):
            txt_files = glob.glob(os.path.join(label_path, "*.txt"))
            all_label_files.extend(txt_files)
    
    if not all_label_files:
        errors.append(f"åœ¨ {dataset_path} ä¸­æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶")
        return False, errors, summary
    
    summary['total_files'] = len(all_label_files)
    
    print(f"[ä¿¡æ¯] æ‰¾åˆ° {len(all_label_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶ï¼Œå¼€å§‹éªŒè¯...")
    
    for i, file_path in enumerate(all_label_files, 1):
        if i % 100 == 0:
            print(f"[è¿›åº¦] å·²éªŒè¯ {i}/{len(all_label_files)} ä¸ªæ–‡ä»¶")
        
        is_valid, file_errors, stats = validate_annotation_file(file_path)
        
        if is_valid:
            summary['valid_files'] += 1
        else:
            summary['invalid_files'] += 1
            errors.extend(file_errors)
        
        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        summary['total_objects'] += stats['valid_lines']
        
        # æ›´æ–°ç±»åˆ«åˆ†å¸ƒ
        for class_id, count in stats['class_ids'].items():
            summary['class_distribution'][class_id] = summary['class_distribution'].get(class_id, 0) + count
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯çš„ç±»åˆ«ID
        if expected_class_id is not None:
            file_class_ids = set(stats['class_ids'].keys())
            if file_class_ids and str(expected_class_id) not in file_class_ids:
                summary['files_with_wrong_class'].append(os.path.basename(file_path))
        
        # æ£€æŸ¥ç©ºæ–‡ä»¶
        if stats['total_lines'] == 0 or (stats['total_lines'] == stats['empty_lines']):
            summary['empty_files'].append(os.path.basename(file_path))
    
    return len(errors) == 0, errors, summary


def print_validation_report(is_valid: bool, errors: List[str], summary: Dict[str, any]) -> None:
    """
    æ‰“å°éªŒè¯æŠ¥å‘Šã€‚
    
    Args:
        is_valid (bool): éªŒè¯ç»“æœã€‚
        errors (List[str]): é”™è¯¯åˆ—è¡¨ã€‚
        summary (Dict[str, any]): æ±‡æ€»ç»Ÿè®¡ã€‚
    """
    print("\n" + "="*60)
    print("YOLOæ ‡æ³¨æ–‡ä»¶éªŒè¯æŠ¥å‘Š")
    print("="*60)
    
    # æ€»ä½“ç»“æœ
    if is_valid:
        print("âœ… [ç»“æœ] éªŒè¯é€šè¿‡")
    else:
        print("âŒ [ç»“æœ] éªŒè¯å¤±è´¥")
    
    # æ–‡ä»¶ç»Ÿè®¡
    print(f"\nğŸ“Š [æ–‡ä»¶ç»Ÿè®¡]")
    print(f"   æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"   æœ‰æ•ˆæ–‡ä»¶: {summary['valid_files']}")
    print(f"   æ— æ•ˆæ–‡ä»¶: {summary['invalid_files']}")
    
    # å¯¹è±¡ç»Ÿè®¡
    print(f"\nğŸ¯ [å¯¹è±¡ç»Ÿè®¡]")
    print(f"   æ€»å¯¹è±¡æ•°: {summary['total_objects']}")
    
    # ç±»åˆ«åˆ†å¸ƒ
    if summary['class_distribution']:
        print(f"\nğŸ·ï¸  [ç±»åˆ«åˆ†å¸ƒ]")
        for class_id, count in sorted(summary['class_distribution'].items(), key=lambda x: int(x[0])):
            print(f"   ç±»åˆ«ID {class_id}: {count} ä¸ªå¯¹è±¡")
    
    # ç‰¹æ®Šæ–‡ä»¶
    if summary['files_with_wrong_class']:
        print(f"\nâš ï¸  [ç±»åˆ«é”™è¯¯æ–‡ä»¶] ({len(summary['files_with_wrong_class'])}ä¸ª)")
        for filename in summary['files_with_wrong_class'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   {filename}")
        if len(summary['files_with_wrong_class']) > 5:
            print(f"   ... è¿˜æœ‰ {len(summary['files_with_wrong_class']) - 5} ä¸ªæ–‡ä»¶")
    
    if summary['empty_files']:
        print(f"\nğŸ“„ [ç©ºæ ‡æ³¨æ–‡ä»¶] ({len(summary['empty_files'])}ä¸ª)")
        for filename in summary['empty_files'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   {filename}")
        if len(summary['empty_files']) > 5:
            print(f"   ... è¿˜æœ‰ {len(summary['empty_files']) - 5} ä¸ªæ–‡ä»¶")
    
    # é”™è¯¯è¯¦æƒ…
    if errors:
        print(f"\nâ— [é”™è¯¯è¯¦æƒ…] ({len(errors)}ä¸ªé”™è¯¯)")
        for error in errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
            print(f"   {error}")
        if len(errors) > 10:
            print(f"   ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯")
    
    # å»ºè®®
    print(f"\nğŸ’¡ [å»ºè®®]")
    if summary['class_distribution'] and len(summary['class_distribution']) > 1:
        print("   âš ï¸  æ£€æµ‹åˆ°å¤šä¸ªç±»åˆ«IDï¼Œå•ç±»åˆ«è®­ç»ƒè¦æ±‚æ‰€æœ‰ç±»åˆ«IDä¸º0")
        print("   ğŸ“‹ å»ºè®®è¿è¡Œ: python validate_annotations.py --fix-class-ids")
    
    if summary['invalid_files'] > 0:
        print("   âš ï¸  å­˜åœ¨æ ¼å¼é”™è¯¯çš„æ ‡æ³¨æ–‡ä»¶")
        print("   ğŸ“‹ è¯·æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶æ ¼å¼æ˜¯å¦ç¬¦åˆYOLOæ ‡å‡†")
    
    if summary['empty_files']:
        print("   âš ï¸  å­˜åœ¨ç©ºæ ‡æ³¨æ–‡ä»¶")
        print("   ğŸ“‹ è¯·ç¡®è®¤è¿™äº›å›¾ç‰‡æ˜¯å¦çœŸçš„æ²¡æœ‰ä»»ä½•ç›®æ ‡å¯¹è±¡")
    
    if is_valid:
        print("   âœ… æ ‡æ³¨æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œè®­ç»ƒ")
    
    print("\n" + "="*60)


def main():
    """
    ä¸»å‡½æ•°ã€‚
    """
    print("YOLOæ ‡æ³¨æ–‡ä»¶éªŒè¯å·¥å…·")
    print("="*60)
    
    # é»˜è®¤æ•°æ®é›†è·¯å¾„
    dataset_path = "train"
    expected_class_id = 0  # é»˜è®¤æœŸæœ›ç±»åˆ«IDä¸º0ï¼ˆå•ç±»åˆ«ï¼‰
    
    if not os.path.exists(dataset_path):
        print(f"[é”™è¯¯] æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        print("[æç¤º] è¯·å…ˆè¿è¡Œ split_dataset.py åˆ›å»ºæ•°æ®é›†")
        return
    
    print(f"[ä¿¡æ¯] éªŒè¯æ•°æ®é›†: {dataset_path}")
    print(f"[ä¿¡æ¯] æœŸæœ›ç±»åˆ«ID: {expected_class_id}")
    
    # éªŒè¯æ•°æ®é›†
    is_valid, errors, summary = validate_dataset_directory(dataset_path, expected_class_id)
    
    # æ‰“å°æŠ¥å‘Š
    print_validation_report(is_valid, errors, summary)
    
    # è¿”å›ç 
    sys.exit(0 if is_valid else 1)


if __name__ == "__main__":
    main()